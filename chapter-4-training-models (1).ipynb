{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "_cell_guid": "b8ad4910-83b0-49a8-a9dd-09faeec10d23",
    "_kg_hide-input": false,
    "_uuid": "73883ff1-9de0-477c-b667-951ec3fa25d5",
    "execution": {
     "iopub.execute_input": "2022-08-19T16:56:20.962801Z",
     "iopub.status.busy": "2022-08-19T16:56:20.962402Z",
     "iopub.status.idle": "2022-08-19T16:56:20.969240Z",
     "shell.execute_reply": "2022-08-19T16:56:20.968514Z",
     "shell.execute_reply.started": "2022-08-19T16:56:20.962735Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a08e9098-058c-42b4-a2e2-e5bce62b14f9",
    "_uuid": "90f15679-f2bd-4e85-a812-746fba1c3a28"
   },
   "source": [
    "#  <b>Training Models - Synopsis<b>\n",
    "### 1. Linear Regression ‚úÖ\n",
    ">      ‚Ä¢ Closed-form \n",
    "     ‚Ä¢ Gradient Descent\n",
    "### 2. Polynomian Regression ‚úÖ\n",
    "### 3. Learning Curves ‚úÖ\n",
    "### 4. Regularized Linear Models‚úÖ\n",
    "\n",
    "### 5. Logistic Regression ‚úÖ\n",
    "### 6. Softmax Regression 113 150 130"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a7aef5d2-a625-46c7-8574-56bc4a517653",
    "_uuid": "4ac69002-630f-429b-be09-2741124758b4"
   },
   "source": [
    "# 1. üìà Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fe7f1f04-dd8a-4776-8511-b60c47394bef",
    "_uuid": "eb9c83d8-3774-4c04-9ebd-2e7391739d2a"
   },
   "source": [
    "## i) Normal Form\n",
    "To find the value of Œ∏ that minimizes the cost function, there is a closed-form solution\n",
    "‚Äîin other words, a mathematical equation that gives the result directly. This is called\n",
    "the Normal Equation\n",
    "\n",
    "#### **Œ∏<sup>^</sup> = (X<sup>T</sup>X )<sup>-1</sup> X<sup>T</sup> y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "_cell_guid": "c4dbccff-06e6-47a8-b980-c7485d3df28c",
    "_uuid": "aa0c0635-8631-40c0-a3a4-b82782640945",
    "execution": {
     "iopub.execute_input": "2022-08-19T16:56:20.971202Z",
     "iopub.status.busy": "2022-08-19T16:56:20.970774Z",
     "iopub.status.idle": "2022-08-19T16:56:21.118541Z",
     "shell.execute_reply": "2022-08-19T16:56:21.117790Z",
     "shell.execute_reply.started": "2022-08-19T16:56:20.971176Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X = 2*np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1) # y = mx + c\n",
    "plt.scatter(X, y, s=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "_cell_guid": "78b01848-5949-4dda-8007-e8fce50ea950",
    "_uuid": "619e9379-1c62-4212-939d-a8578d1c4478",
    "execution": {
     "iopub.execute_input": "2022-08-19T16:56:21.120060Z",
     "iopub.status.busy": "2022-08-19T16:56:21.119657Z",
     "iopub.status.idle": "2022-08-19T16:56:21.124509Z",
     "shell.execute_reply": "2022-08-19T16:56:21.123912Z",
     "shell.execute_reply.started": "2022-08-19T16:56:21.120032Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "_cell_guid": "189f44a3-23b2-4333-8e74-b1ac5383287a",
    "_uuid": "6ffbed13-d8ff-48e5-a5b6-c3cccc04a933",
    "execution": {
     "iopub.execute_input": "2022-08-19T16:56:21.126667Z",
     "iopub.status.busy": "2022-08-19T16:56:21.126247Z",
     "iopub.status.idle": "2022-08-19T16:56:21.261426Z",
     "shell.execute_reply": "2022-08-19T16:56:21.260735Z",
     "shell.execute_reply.started": "2022-08-19T16:56:21.126643Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = X_b.dot(theta_best)\n",
    "plt.scatter(X, y, s=7)\n",
    "plt.plot(X, y_pred, c=\"orange\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "_cell_guid": "30ccf30e-844a-48b7-a113-ac21c020d34e",
    "_uuid": "e359821a-89ca-4011-978a-4a9198772794",
    "execution": {
     "iopub.execute_input": "2022-08-19T16:56:21.262961Z",
     "iopub.status.busy": "2022-08-19T16:56:21.262527Z",
     "iopub.status.idle": "2022-08-19T16:56:21.268162Z",
     "shell.execute_reply": "2022-08-19T16:56:21.267508Z",
     "shell.execute_reply.started": "2022-08-19T16:56:21.262932Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "201cac0a-5ce2-4f82-b064-d67e101728a7",
    "_uuid": "cb101818-082a-473e-92c0-b89606b29f52"
   },
   "source": [
    "#### üëâ Other Methods to Calculate \"theta_best\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "_cell_guid": "ca85df4e-e00b-4e13-b145-c6394e5e8aec",
    "_uuid": "af9b57ee-53ae-4cfe-8937-db2e9dd48831",
    "execution": {
     "iopub.execute_input": "2022-08-19T16:56:21.270045Z",
     "iopub.status.busy": "2022-08-19T16:56:21.269661Z",
     "iopub.status.idle": "2022-08-19T16:56:21.281406Z",
     "shell.execute_reply": "2022-08-19T16:56:21.280576Z",
     "shell.execute_reply.started": "2022-08-19T16:56:21.270013Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 1. Using Sklearn LinearRegression()\n",
    "\n",
    "# The LinearRegression class is based on the \n",
    "# scipy.linalg.lstsq() function (the name stands for \n",
    "# ‚Äúleast squares‚Äù)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "print( lin_reg.intercept_, lin_reg.coef_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "_cell_guid": "5f97e987-93dc-45b5-b7ac-24800445cc75",
    "_uuid": "ebad00ea-ef3e-4e85-8a32-0d57dcdb7187",
    "execution": {
     "iopub.execute_input": "2022-08-19T16:56:21.283771Z",
     "iopub.status.busy": "2022-08-19T16:56:21.282544Z",
     "iopub.status.idle": "2022-08-19T16:56:21.296894Z",
     "shell.execute_reply": "2022-08-19T16:56:21.296006Z",
     "shell.execute_reply.started": "2022-08-19T16:56:21.283712Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 2. Using Numpy Least Square Method\n",
    "\n",
    "theta_best_svd, residuals, rank, s= np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "_cell_guid": "16349566-3a1f-4b6b-8ed1-c4092c765002",
    "_uuid": "6241c835-3eef-4ca7-8f28-5ef26d34a290",
    "execution": {
     "iopub.execute_input": "2022-08-19T16:56:21.299936Z",
     "iopub.status.busy": "2022-08-19T16:56:21.298167Z",
     "iopub.status.idle": "2022-08-19T16:56:21.310110Z",
     "shell.execute_reply": "2022-08-19T16:56:21.308045Z",
     "shell.execute_reply.started": "2022-08-19T16:56:21.299871Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 3. Using Numpy PseudoInverse of X and dot(y)\n",
    "\n",
    "# This approach is more efficient than computing the\n",
    "# Normal Equation, plus it handles edge cases nicely: \n",
    "# indeed, the Normal Equation may not work if the matrix \n",
    "# XTX(X transpose * X) is not invertible (i.e., singular),  \n",
    "# such as if m < n or if some features are redundant, but the \n",
    "# pseudoinverse is always defined.\n",
    "\n",
    "\n",
    "np.linalg.pinv(X_b).dot(y)\n",
    "\n",
    "# The pseudoinverse itself is computed using a standard \n",
    "# matrix factorization technique called Singular \n",
    "# Value Decomposition (SVD) that can decompose the \n",
    "# training set matrix X into the matrix multiplication \n",
    "# of three matrices U Œ£ V^T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "84ba1322-9a6e-4477-9605-2e286ce7e42b",
    "_uuid": "3fc0bb52-0be0-4577-8de5-813c27b9d2a7"
   },
   "source": [
    "#### Computation Complexity\n",
    "The Normal Equation computes the inverse of X<sup>T </sup> X, which is an (n + 1) √ó (n + 1)\n",
    "matrix (where n is the number of features). The computational complexity of inverting\n",
    "such a matrix is typically about O(n<sup>2.4</sup>) to O(n<sup>3</sup>) (depending on the implementation).\n",
    "In other words, if you double the number of features, you multiply the computation\n",
    "time by roughly 2<sup>2.4</sup> = 5.3 to 2<sup>3</sup> = 8.\n",
    "\n",
    "üëâ *Both the Normal Equation and the SVD approach get very slow\n",
    "when the number of features grows large (e.g., 100,000). On the\n",
    "positive side, both are linear with regards to the number of instances\n",
    "in the training set (they are O(m)), so they handle large training\n",
    "sets efficiently, provided they can fit in memory.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bce8c610-6815-4af6-b13e-86932cf1fe8d",
    "_uuid": "9a022f82-32e5-4f7f-86bb-5f794a6908b4"
   },
   "source": [
    "## ii) Gradient Descent\n",
    "The general idea of Gradient Descent is to\n",
    "tweak parameters iteratively in order to minimize a cost function.<br>\n",
    "The cost function has the shape of a bowl, but it can be an elongated bowl if\n",
    "the features have very different scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7f071b15-66a2-4834-b4f8-d9a855136c7a",
    "_uuid": "b8442978-1574-4685-9c95-d487214a1ed0"
   },
   "source": [
    "#### a) Batch Gradient Descent\n",
    "It is simply the derivative term, which is like asking \"what is the slope of the mountain uder my feet if I face east?..and the west and so on..for all directions.\n",
    "\n",
    "Notice that this formula involves calculations over the full training\n",
    "set X, at each Gradient Descent step! This is why the algorithm is\n",
    "called *Batch Gradient Descent*: it uses the whole batch of training\n",
    "data at every step . As a result it is terribly slow on very large training\n",
    "sets . Training a Linear Regression model when there are hundreds\n",
    "of thousands of features is much faster using Gradient\n",
    "Descent than using the Normal Equation or SVD decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "41fcbe08-3af7-4b69-8dca-e9b966959e58",
    "_uuid": "8cb9ad12-a9cd-4655-81c6-f5e8e45bab90"
   },
   "source": [
    "##### üëâ Gradient Descent step\n",
    "\n",
    "#### **Œ∏** <sup>(next step)</sup> = **Œ∏** ‚àí Œ∑ ‚àá<sub>Œ∏</sub>MSE(**Œ∏**)\n",
    "Once you have the gradient vector, which points uphill, just go in the opposite direction\n",
    "to go downhill. This means subtracting ‚àá<sub>Œ∏</sub>MSE(**Œ∏**) from **Œ∏**. This is where the\n",
    "learning rate Œ∑ comes into play: multiply the gradient vector by Œ∑ to determine the\n",
    "size of the downhill step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "_cell_guid": "a1bca9a4-8eb8-4d77-8f0f-8f48bcb18a1b",
    "_uuid": "174ede17-ce6a-43f7-a4b7-04dcd41f3b82",
    "execution": {
     "iopub.execute_input": "2022-08-19T16:56:21.313738Z",
     "iopub.status.busy": "2022-08-19T16:56:21.313183Z",
     "iopub.status.idle": "2022-08-19T16:56:21.353446Z",
     "shell.execute_reply": "2022-08-19T16:56:21.352593Z",
     "shell.execute_reply.started": "2022-08-19T16:56:21.313711Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "eta = 0.1 #learning rate\n",
    "n_iterations = 1000\n",
    "m = X_b.shape[0]\n",
    "bt_theta=np.array([0,0]) #array storing regression values\n",
    "theta = [[0],[0]]\n",
    "for iteration in range(n_iterations):\n",
    "    mse = (2/m)*(X_b.T.dot(X_b.dot(theta) - y))\n",
    "    theta = theta - eta * mse\n",
    "    bt_theta = np.c_[bt_theta, theta] #data plotted at last\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fc9287b6-bcc6-4cc5-b47d-784355f4e498",
    "_uuid": "6a805f7b-5147-4f0a-b178-7669ea43d912"
   },
   "source": [
    "Note: *To find a good learning rate, you can use grid search (see Chapter 2). However, you\n",
    "may want to limit the number of iterations so that grid search can eliminate models\n",
    "that take too long to converge.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "395d1a8e-bce5-4256-bb26-eb711ac199e9",
    "_uuid": "7e978917-dbac-4415-b369-e3678633d378"
   },
   "source": [
    "#### b) Stochastic Gradient Descent (SGD)\n",
    "Batch Gradient Descent uses the whole\n",
    "training set to compute the gradients at every step, which makes it very slow when\n",
    "the training set is large. At the opposite extreme, Stochastic Gradient Descent just\n",
    "picks a random instance in the training set at every step and computes the gradients\n",
    "based only on that single instance. Obviously this makes the algorithm much faster\n",
    "since it has very little data to manipulate at every iteration. It also makes it possible to\n",
    "train on huge training sets, since only one instance needs to be in memory at each\n",
    "iteration.<br>\n",
    "On the other hand, due to its stochastic (i.e., random) nature, this algorithm is much\n",
    "less regular than Batch Gradient Descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "_cell_guid": "126a94ba-0d1d-4a69-920f-65b5fb5dd5b0",
    "_uuid": "3974cf35-0621-4f97-83f9-99f4705b86e3",
    "execution": {
     "iopub.execute_input": "2022-08-19T16:56:21.355328Z",
     "iopub.status.busy": "2022-08-19T16:56:21.354796Z",
     "iopub.status.idle": "2022-08-19T16:56:21.425011Z",
     "shell.execute_reply": "2022-08-19T16:56:21.424025Z",
     "shell.execute_reply.started": "2022-08-19T16:56:21.355295Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "t0, t1 = 5, 10\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "sc_theta = np.array([0,0])\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index: random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta *  gradients\n",
    "    sc_theta = np.c_[sc_theta,theta]\n",
    "\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cc6f7bae-4d8f-464b-88e0-dc3de36dd9cd",
    "_uuid": "62365c15-06ab-403b-ab68-be23cc56a531"
   },
   "source": [
    "##### Perform SGD using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "_cell_guid": "a77d7569-3a5e-4189-95ea-87e000e535b5",
    "_uuid": "f29a3103-8c43-42bc-8077-cb37cb35e9fd",
    "execution": {
     "iopub.execute_input": "2022-08-19T16:56:21.426531Z",
     "iopub.status.busy": "2022-08-19T16:56:21.425886Z",
     "iopub.status.idle": "2022-08-19T16:56:21.432549Z",
     "shell.execute_reply": "2022-08-19T16:56:21.431932Z",
     "shell.execute_reply.started": "2022-08-19T16:56:21.426505Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\n",
    "sgd_reg.fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "_cell_guid": "02bab3ba-64f0-46f1-a461-98f4db9390d2",
    "_uuid": "a7c29342-eb85-46f3-b680-1426bcbacf46",
    "execution": {
     "iopub.execute_input": "2022-08-19T16:56:21.433679Z",
     "iopub.status.busy": "2022-08-19T16:56:21.433427Z",
     "iopub.status.idle": "2022-08-19T16:56:21.445525Z",
     "shell.execute_reply": "2022-08-19T16:56:21.444459Z",
     "shell.execute_reply.started": "2022-08-19T16:56:21.433656Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6c0a08c2-3bdb-4676-a62a-32f1064bb85e",
    "_uuid": "9453f1d8-9d40-4718-8f07-417ec67da988"
   },
   "source": [
    "#### c) Mini-batch Gradient Descent\n",
    "at each step, instead of computing the gradients based on the full training\n",
    "set (as in Batch GD) or based on just one instance (as in Stochastic GD), Minibatch\n",
    "GD computes the gradients on small random sets of instances called minibatches.\n",
    "The main advantage of Mini-batch GD over Stochastic GD is that you can\n",
    "get a performance boost from hardware optimization of matrix operations, especially\n",
    "when using GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "_cell_guid": "510dbf00-19be-4c93-88c3-8ff5f9862c81",
    "_uuid": "373d094d-dca0-409c-9cd5-121fd7111b73",
    "execution": {
     "iopub.execute_input": "2022-08-19T16:56:21.447063Z",
     "iopub.status.busy": "2022-08-19T16:56:21.446804Z",
     "iopub.status.idle": "2022-08-19T16:56:21.608963Z",
     "shell.execute_reply": "2022-08-19T16:56:21.607855Z",
     "shell.execute_reply.started": "2022-08-19T16:56:21.447033Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(sc_theta[0,1:], sc_theta[1,1:],\"rs-\", label=\"Stochastic\",linewidth=1)\n",
    "plt.plot(bt_theta[0,10:], bt_theta[1,10:],\"bo-\", label=\"Batch\",linewidth=1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0ad0e649-3e3f-4f84-99cd-dab6e1490f58",
    "_uuid": "2b9a665f-5389-43f5-9b9e-d0bb1b8a8556"
   },
   "source": [
    "<table><tr><th>Algorithm</th> <th>Large m</th> <th>Out-of-core support</th> <th>Large n</th> <th> Hyperparams</th> <th>Scaling required</th> <th> Scikit-Learn</th></tr>\n",
    "<tr><td>Normal Equation</td> <td> Fast </td> <td> No</td><td> Slow </td><td>0</td><td> No</td><td> n/a</td></tr>\n",
    "<tr><td>SVD</td><td> Fast</td><td> No</td><td> Slow</td><td> 0 </td><td>No</td><td> LinearRegression</td></tr>\n",
    "<td>Batch GD</td><td> Slow </td><td>No</td><td> Fast</td><td> 2</td><td> Yes</td><td> SGDRegressor</td>\n",
    "<tr><td>Stochastic GD</td><td> Fast</td><td> Yes</td><td> Fast</td><td> ‚â•2</td><td> Yes</td><td> SGDRegressor</td></tr>\n",
    "<tr><td>Mini-batch GD</td><td> Fast</td><td> Yes </td><td>Fast</td><td> ‚â•2</td><td> Yes</td><td> SGDRegressor</td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "49ec5bc9-88bb-4079-a078-27f961f509e0",
    "_uuid": "24401438-6036-4358-9ae2-570b06f89ce2"
   },
   "source": [
    "# 2. ‚§¥Ô∏è Polynomial Regression\n",
    "Surprisingly, a simple way to use a linear model to fit nonlinear data is to\n",
    "add powers of each feature as new features, then train a linear model on this extended\n",
    "set of features. This technique is called *Polynomial Regression.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T16:56:21.610658Z",
     "iopub.status.busy": "2022-08-19T16:56:21.610357Z",
     "iopub.status.idle": "2022-08-19T16:56:21.617034Z",
     "shell.execute_reply": "2022-08-19T16:56:21.615806Z",
     "shell.execute_reply.started": "2022-08-19T16:56:21.610624Z"
    }
   },
   "outputs": [],
   "source": [
    "# lets take a test dataset\n",
    "m = 200\n",
    "X = 6 * np.random.rand(m, 1) - 3 \n",
    "y = 0.5 * X**2 + X + 2 +  np.random.randn(m, 1) \n",
    "\n",
    "# here, random.randn is used to include some noice in the data\n",
    "# try to remove this term to get more understanding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T16:56:21.618469Z",
     "iopub.status.busy": "2022-08-19T16:56:21.618248Z",
     "iopub.status.idle": "2022-08-19T16:56:21.755123Z",
     "shell.execute_reply": "2022-08-19T16:56:21.754355Z",
     "shell.execute_reply.started": "2022-08-19T16:56:21.618446Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X, y, alpha=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding 2nd Degree Polynommial as new feature\n",
    "As a straight line will never fit this data properly. So, use scikit-learn's to transform our training data, adding the square(2nd degree polynomial) of each feature in the traiing set as new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T16:56:21.757095Z",
     "iopub.status.busy": "2022-08-19T16:56:21.756366Z",
     "iopub.status.idle": "2022-08-19T16:56:21.765085Z",
     "shell.execute_reply": "2022-08-19T16:56:21.764040Z",
     "shell.execute_reply.started": "2022-08-19T16:56:21.757062Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias= False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "X[0], X_poly[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T16:56:21.766858Z",
     "iopub.status.busy": "2022-08-19T16:56:21.766108Z",
     "iopub.status.idle": "2022-08-19T16:56:21.778516Z",
     "shell.execute_reply": "2022-08-19T16:56:21.777781Z",
     "shell.execute_reply.started": "2022-08-19T16:56:21.766827Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now training a linear model on polynomial function \n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T16:56:21.780683Z",
     "iopub.status.busy": "2022-08-19T16:56:21.779504Z",
     "iopub.status.idle": "2022-08-19T16:56:21.925334Z",
     "shell.execute_reply": "2022-08-19T16:56:21.924672Z",
     "shell.execute_reply.started": "2022-08-19T16:56:21.780622Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_test = np.arange(-3, 3.6, 0.2)\n",
    "pred_line = []\n",
    "for i in range(len(pred_test)):\n",
    "    pred_line.append(lin_reg.coef_[0, 0] * pred_test[i] +  lin_reg.coef_[0, 1] * pred_test[i]**2 +  lin_reg.intercept_ )\n",
    "plt.plot(pred_test, pred_line, \"r-\", linewidth = 4, alpha=0.9)\n",
    "plt.scatter(X, y, alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note that when there are multiple features, Polynomial Regression is capable of finding\n",
    "relationships between features (which is something a plain Linear Regression\n",
    "model cannot do). This is made possible by the fact that PolynomialFeatures also\n",
    "adds all combinations of features up to the given degree. For example, if there were\n",
    "two features a and b, PolynomialFeatures with degree=3 would not only add the\n",
    "features a2, a3, b2, and b3, but also the combinations ab, a2b, and ab2.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) üåäLearning Curve\n",
    "In Chapter 2 you used cross-validation to get an estimate of a model‚Äôs generalization\n",
    "performance.\n",
    "Another way is to look at the learning curves: these are plots of the model‚Äôs performance\n",
    "on the training set and the validation set as a function of the training set size\n",
    "(or the training iteration). To generate the plots, simply train the model several times\n",
    "on different sized subsets of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "_cell_guid": "071e7887-5937-4aad-a129-de5621a6eb7f",
    "_uuid": "b7ed7e5a-1b46-4c13-b6e0-177d32bbe60b",
    "execution": {
     "iopub.execute_input": "2022-08-19T16:56:21.927047Z",
     "iopub.status.busy": "2022-08-19T16:56:21.926316Z",
     "iopub.status.idle": "2022-08-19T16:56:21.934431Z",
     "shell.execute_reply": "2022-08-19T16:56:21.933389Z",
     "shell.execute_reply.started": "2022-08-19T16:56:21.927016Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def plot_learning_curve(model, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    train_error = []\n",
    "    test_error = []\n",
    "    for a in range(1, len(X_train)):\n",
    "        model.fit(X_train[:a], y_train[:a])\n",
    "        X_train_pred = model.predict(X_train[:a])\n",
    "        X_test_pred = model.predict(X_test)\n",
    "        train_error.append(mean_squared_error(y_train[:a], X_train_pred))\n",
    "        test_error.append(mean_squared_error(y_test, X_test_pred))\n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T16:56:21.936389Z",
     "iopub.status.busy": "2022-08-19T16:56:21.936089Z",
     "iopub.status.idle": "2022-08-19T16:56:22.223285Z",
     "shell.execute_reply": "2022-08-19T16:56:22.220814Z",
     "shell.execute_reply.started": "2022-08-19T16:56:21.936365Z"
    }
   },
   "outputs": [],
   "source": [
    "train_error, test_error = plot_learning_curve(lin_reg, X_poly, y)\n",
    "plt.plot(np.sqrt(train_error[8:]), \"r-+\", linewidth=2, label=\"train\")\n",
    "plt.plot(np.sqrt(test_error[8:]), \"b-\", linewidth = 3, label=\"test\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MeanSquaredError\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "04a050f9-d649-46e0-a46d-bde67c72ba3f",
    "_uuid": "31596ef6-8939-426e-99c2-514b15a52636",
    "execution": {
     "iopub.execute_input": "2022-08-19T03:14:39.226451Z",
     "iopub.status.busy": "2022-08-19T03:14:39.225951Z",
     "iopub.status.idle": "2022-08-19T03:14:39.357677Z",
     "shell.execute_reply": "2022-08-19T03:14:39.356083Z",
     "shell.execute_reply.started": "2022-08-19T03:14:39.226412Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### ‚ö†Ô∏è CAUTION ! \n",
    "> If your model is **underfitting** the training data, adding more training\n",
    "examples will not help. You need to use a more complex model\n",
    "or come up with better features.<br>\n",
    "> One way to improve an **overfitting** model is to feed it more training\n",
    "data until the validation error reaches the training error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bias/Variance Tradeoff\n",
    "An important theoretical result of statistics and Machine Learning is the fact that a\n",
    "model‚Äôs generalization error can be expressed as the sum of three very different\n",
    "errors:\n",
    "#### Bias\n",
    "This part of the generalization error is due to wrong assumptions, such as assuming\n",
    "that the data is linear when it is actually quadratic. A high-bias model is most\n",
    "likely to underfit the training data.10\n",
    "#### Variance\n",
    "This part is due to the model‚Äôs excessive sensitivity to small variations in the\n",
    "training data. A model with many degrees of freedom (such as a high-degree polynomial\n",
    "model) is likely to have high variance, and thus to overfit the training\n",
    "data.\n",
    "#### Irreducible error\n",
    "This part is due to the noisiness of the data itself. The only way to reduce this\n",
    "part of the error is to clean up the data (e.g., fix the data sources, such as broken\n",
    "sensors, or detect and remove outliers).\n",
    "Increasing a model‚Äôs complexity will typically increase its variance and reduce its bias.\n",
    "Conversely, reducing a model‚Äôs complexity increases its bias and reduces its variance.\n",
    "This is why it is called a tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) üìá Regularized Linear Models\n",
    "a good way to reduce overfitting is to regularize the\n",
    "model (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be\n",
    "for it to overfit the data. For example, a simple way to regularize a polynomial model\n",
    "is to reduce the number of polynomial degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Ridge Regression\n",
    "Ridge Regression (also called Tikhonov regularization) is a regularized version of Linear\n",
    "Regression: a regularization term equal to <br> **Œ±Œ£<sup>n</sup><sub>i = 1</sub>\n",
    " Œ∏<sub>i</sub><sup>2</sup>** <br> is added to the cost function.\n",
    "This forces the learning algorithm to not only fit the data but also keep the model\n",
    "weights as small as possible. Note that the regularization term should only be added\n",
    "to the cost function during training.<br>\n",
    "<h4> Ridge Regression Cost Form: </h4>\n",
    "\n",
    "#### **J(Œ∏) = MSE(Œ∏) + Œ±Œ£<sup>n</sup><sub>i=1 </sub> Œ∏<sub>i</sub><sup>2</sup>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T16:56:22.225037Z",
     "iopub.status.busy": "2022-08-19T16:56:22.224580Z",
     "iopub.status.idle": "2022-08-19T16:56:22.405781Z",
     "shell.execute_reply": "2022-08-19T16:56:22.405112Z",
     "shell.execute_reply.started": "2022-08-19T16:56:22.225007Z"
    }
   },
   "outputs": [],
   "source": [
    "#Perform Ridge Regression using Ridge Model\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha=1000, solver=\"cholesky\")\n",
    "ridge_reg.fit(X, y)\n",
    "ridge_pred = ridge_reg.predict(pred_test.reshape(-1, 1))\n",
    "plt.plot(pred_test, ridge_pred, c=\"r\", label=\"alpha=1000\")\n",
    "\n",
    "ridge_reg = Ridge(alpha=100, solver=\"cholesky\")\n",
    "ridge_reg.fit(X, y)\n",
    "ridge_pred = ridge_reg.predict(pred_test.reshape(-1, 1))\n",
    "plt.plot(pred_test, ridge_pred, c=\"g\", label=\"alpha=100\")\n",
    "\n",
    "ridge_reg = Ridge(alpha=0, solver=\"cholesky\")\n",
    "ridge_reg.fit(X, y)\n",
    "ridge_pred = ridge_reg.predict(pred_test.reshape(-1, 1))\n",
    "plt.plot(pred_test, ridge_pred, c=\"b\", label=\"alpha=0\")\n",
    "\n",
    "plt.legend()\n",
    "plt.plot(pred_test, ridge_pred)\n",
    "plt.scatter(X, y, alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T16:56:22.406991Z",
     "iopub.status.busy": "2022-08-19T16:56:22.406641Z",
     "iopub.status.idle": "2022-08-19T16:56:22.414926Z",
     "shell.execute_reply": "2022-08-19T16:56:22.413280Z",
     "shell.execute_reply.started": "2022-08-19T16:56:22.406968Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ridge Regression using SGDRegressor panelty=\"l2\"\n",
    "\n",
    "sgd_reg = SGDRegressor( penalty=\"l2\")\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "sgd_reg.predict([[-3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T16:56:22.421084Z",
     "iopub.status.busy": "2022-08-19T16:56:22.420326Z",
     "iopub.status.idle": "2022-08-19T16:56:22.626907Z",
     "shell.execute_reply": "2022-08-19T16:56:22.625814Z",
     "shell.execute_reply.started": "2022-08-19T16:56:22.421056Z"
    }
   },
   "outputs": [],
   "source": [
    "# making it degree 10 features\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=10, include_bias= False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "X[0], X_poly[0]\n",
    "\n",
    "# scaling the features\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_10_scaled = scaler.fit_transform(X_poly.astype(np.float64))\n",
    "\n",
    "# Ridge Regression Applied\n",
    "\n",
    "ridge_reg = Ridge(alpha=1e-100, solver=\"cholesky\")\n",
    "ridge_reg.fit(X_10_scaled, y)\n",
    "ridge_pred_10 = ridge_reg.predict(X_10_scaled)\n",
    "plt.scatter(X, ridge_pred_10, s=5, c=\"r\", label=\"alpha=1e-100\")\n",
    "\n",
    "ridge_reg = Ridge(alpha=100, solver=\"cholesky\")\n",
    "ridge_reg.fit(X_10_scaled, y)\n",
    "ridge_pred_10 = ridge_reg.predict(X_10_scaled)\n",
    "plt.scatter(X, ridge_pred_10, s=3, c=\"b\", label=\"alpha=100\")\n",
    "\n",
    "ridge_reg = Ridge(alpha=1, solver=\"cholesky\")\n",
    "ridge_reg.fit(X_10_scaled, y)\n",
    "ridge_pred_10 = ridge_reg.predict(X_10_scaled)\n",
    "plt.scatter(X, ridge_pred_10, s=3, c=\"g\", label=\"alpha=1\")\n",
    "\n",
    "plt.legend()\n",
    "plt.scatter(X, y, alpha=0.2)\n",
    "\n",
    "#increasing the alpha will increase the overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Lasso Regression\n",
    "Least Absolute Shrinkage and Selection Operator Regression (simply called Lasso\n",
    "Regression) is another regularized version of Linear Regression: just like Ridge\n",
    "Regression, it adds a regularization term to the cost function, but it uses the ‚Ñì1 norm\n",
    "of the weight vector instead of half the square of the ‚Ñì2 norm\n",
    "\n",
    "<h4> Lasso Regression Cost Form: </h4>\n",
    "\n",
    "#### **J(Œ∏) = MSE(Œ∏) + Œ±Œ£<sup>n</sup><sub>i = 1</sub>|Œ∏<sub>i</sub>|**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T16:56:22.628696Z",
     "iopub.status.busy": "2022-08-19T16:56:22.628423Z",
     "iopub.status.idle": "2022-08-19T16:56:22.781675Z",
     "shell.execute_reply": "2022-08-19T16:56:22.780563Z",
     "shell.execute_reply.started": "2022-08-19T16:56:22.628672Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso_reg = Lasso(alpha=.2)\n",
    "lasso_reg.fit(X_10_scaled, y)\n",
    "\n",
    "lasso_pred = lasso_reg.predict(X_10_scaled)\n",
    "plt.scatter(X, lasso_pred, c=\"r\", marker=\"+\", label=\"alpha=0.2\")\n",
    "plt.scatter(X, y, alpha=0.2)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T16:56:22.783018Z",
     "iopub.status.busy": "2022-08-19T16:56:22.782777Z",
     "iopub.status.idle": "2022-08-19T16:56:22.786189Z",
     "shell.execute_reply": "2022-08-19T16:56:22.785598Z",
     "shell.execute_reply.started": "2022-08-19T16:56:22.782995Z"
    }
   },
   "outputs": [],
   "source": [
    "# can also be applied using SGDRegressor(penalty=\"l1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Elastic Net\n",
    "Elastic Net is a middle ground between Ridge Regression and Lasso Regression. The\n",
    "regularization term is a simple mix of both Ridge and Lasso‚Äôs regularization terms,\n",
    "and you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge\n",
    "Regression, and when r = 1, it is equivalent to Lasso Regression\n",
    "#### **J(Œ∏) = MSE(Œ∏) + Œ±Œ£<sup>n</sup><sub>i = 1</sub>|Œ∏<sub>i</sub>| + (1-r)/2 *  Œ±Œ£<sup>n</sup><sub>i=1 </sub> Œ∏<sub>i</sub><sup>2</sup>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T16:56:22.788066Z",
     "iopub.status.busy": "2022-08-19T16:56:22.786990Z",
     "iopub.status.idle": "2022-08-19T16:56:22.942489Z",
     "shell.execute_reply": "2022-08-19T16:56:22.941344Z",
     "shell.execute_reply.started": "2022-08-19T16:56:22.788007Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "elastic_net = ElasticNet(alpha=0.8, l1_ratio=0.5)\n",
    "elastic_net.fit(X_10_scaled, y)\n",
    "elastic_pred = elastic_net.predict(X_10_scaled)\n",
    "\n",
    "plt.scatter(X, elastic_pred, c=\"r\", marker=\"+\", label=f\"alpha={elastic_net.alpha}\")\n",
    "plt.scatter(X, y, alpha=0.2)\n",
    "plt.legend()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) üö´Early Stopping\n",
    "A very different way to regularize iterative learning algorithms such as Gradient\n",
    "Descent is to stop training as soon as the validation error reaches a minimum. This is\n",
    "called early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T17:01:01.362927Z",
     "iopub.status.busy": "2022-08-19T17:01:01.362606Z",
     "iopub.status.idle": "2022-08-19T17:01:01.809541Z",
     "shell.execute_reply": "2022-08-19T17:01:01.808926Z",
     "shell.execute_reply.started": "2022-08-19T17:01:01.362903Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "#prepare the data\n",
    "\n",
    "poly_scaled = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
    "    (\"std_scaler\", StandardScaler())\n",
    "])\n",
    "X_train_poly_scaled = poly_scaled.fit_transform(X_train)\n",
    "X_val_poly_scaled = poly_scaled.transform(X_test)\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True, penalty=None, learning_rate=\"constant\", eta0=0.0005)\n",
    "\n",
    "minimum_val_error = float(\"inf\")\n",
    "best_epoch = None\n",
    "best_model = None\n",
    "for epoch in range(1000):\n",
    "    sgd_reg.fit(X_train_poly_scaled, y_train.ravel())\n",
    "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
    "    val_error = mean_squared_error(y_test, y_val_predict)\n",
    "   \n",
    "    if val_error < minimum_val_error:\n",
    "        minimum_val_error = val_error\n",
    "        best_epoch = epoch\n",
    "        best_model = clone(sgd_reg)\n",
    "        best_prediction = y_val_predict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T17:01:03.518963Z",
     "iopub.status.busy": "2022-08-19T17:01:03.518187Z",
     "iopub.status.idle": "2022-08-19T17:01:03.658321Z",
     "shell.execute_reply": "2022-08-19T17:01:03.657433Z",
     "shell.execute_reply.started": "2022-08-19T17:01:03.518924Z"
    }
   },
   "outputs": [],
   "source": [
    "best_model.fit(X_val_poly_scaled, y_test.ravel())\n",
    "prediction = best_model.predict(X_val_poly_scaled)\n",
    "plt.scatter(X_test, y_test)\n",
    "plt.scatter(X_test, best_prediction)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T17:01:07.698023Z",
     "iopub.status.busy": "2022-08-19T17:01:07.697689Z",
     "iopub.status.idle": "2022-08-19T17:01:07.704475Z",
     "shell.execute_reply": "2022-08-19T17:01:07.703606Z",
     "shell.execute_reply.started": "2022-08-19T17:01:07.697998Z"
    }
   },
   "outputs": [],
   "source": [
    "best_epoch, minimum_val_error, mean_squared_error(y_test, best_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) ü™µLogistic Regression\n",
    "Logistic Regression (also called Logit Regression) is commonly\n",
    "used to estimate the probability that an instance belongs to a particular class\n",
    "(e.g., what is the probability that this email is spam?). If the estimated probability is\n",
    "greater than 50%, then the model predicts that the instance belongs to that class\n",
    "(called the positive class, labeled ‚Äú1‚Äù), or else it predicts that it does not (i.e., it\n",
    "belongs to the negative class, labeled ‚Äú0‚Äù). This makes it a binary classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T17:11:43.591695Z",
     "iopub.status.busy": "2022-08-19T17:11:43.591303Z",
     "iopub.status.idle": "2022-08-19T17:11:43.718413Z",
     "shell.execute_reply": "2022-08-19T17:11:43.717597Z",
     "shell.execute_reply.started": "2022-08-19T17:11:43.591668Z"
    }
   },
   "outputs": [],
   "source": [
    "y = np.arange(-10, 10, 0.4)\n",
    "X = 1/(1+np.exp(-y))\n",
    "plt.plot(y, X,  \"b\", label=(\"sigmoid fun.\"))\n",
    "plt.grid(\"on\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundary\n",
    "Let‚Äôs use the iris dataset to illustrate Logistic Regression. This is a famous dataset that\n",
    "contains the sepal and petal length and width of 150 iris flowers of three different\n",
    "species: Iris-Setosa, Iris-Versicolor, and Iris-Virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T17:15:00.922043Z",
     "iopub.status.busy": "2022-08-19T17:15:00.921707Z",
     "iopub.status.idle": "2022-08-19T17:15:00.928380Z",
     "shell.execute_reply": "2022-08-19T17:15:00.927722Z",
     "shell.execute_reply.started": "2022-08-19T17:15:00.922017Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "list(iris.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T17:16:14.992364Z",
     "iopub.status.busy": "2022-08-19T17:16:14.992039Z",
     "iopub.status.idle": "2022-08-19T17:16:14.996723Z",
     "shell.execute_reply": "2022-08-19T17:16:14.996051Z",
     "shell.execute_reply.started": "2022-08-19T17:16:14.992339Z"
    }
   },
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, 3:] # petal_width\n",
    "y = (iris[\"target\"] == 2).astype(int) # If Iris_Virginica, else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T17:18:28.043313Z",
     "iopub.status.busy": "2022-08-19T17:18:28.043002Z",
     "iopub.status.idle": "2022-08-19T17:18:28.054348Z",
     "shell.execute_reply": "2022-08-19T17:18:28.053656Z",
     "shell.execute_reply.started": "2022-08-19T17:18:28.043289Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T17:23:08.503601Z",
     "iopub.status.busy": "2022-08-19T17:23:08.503281Z",
     "iopub.status.idle": "2022-08-19T17:23:08.665154Z",
     "shell.execute_reply": "2022-08-19T17:23:08.664253Z",
     "shell.execute_reply.started": "2022-08-19T17:23:08.503576Z"
    }
   },
   "outputs": [],
   "source": [
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", label=\"Iris_Virginica\")\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", label=\"Not-Iris-Virginica\")\n",
    "plt.scatter(X, y, marker=\"+\", c=\"red\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üóíÔ∏èNote:\n",
    ">The hyperparameter controlling the regularization strength of a\n",
    "Scikit-Learn LogisticRegression model is not alpha (as in other\n",
    "linear models), but its inverse: C. The higher the value of C, the less\n",
    "the model is regularized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) ü•§ Softmax Regression\n",
    "The Logistic Regression model can be generalized to support multiple classes directly,\n",
    "without having to train and combine multiple binary classifiers . This is called Softmax Regression, or Multinomial Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T17:30:13.567126Z",
     "iopub.status.busy": "2022-08-19T17:30:13.566716Z",
     "iopub.status.idle": "2022-08-19T17:30:13.589521Z",
     "shell.execute_reply": "2022-08-19T17:30:13.588434Z",
     "shell.execute_reply.started": "2022-08-19T17:30:13.567061Z"
    }
   },
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
    "y = iris[\"target\"]\n",
    "\n",
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10)\n",
    "softmax_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T17:30:46.102430Z",
     "iopub.status.busy": "2022-08-19T17:30:46.102130Z",
     "iopub.status.idle": "2022-08-19T17:30:46.109129Z",
     "shell.execute_reply": "2022-08-19T17:30:46.108153Z",
     "shell.execute_reply.started": "2022-08-19T17:30:46.102407Z"
    }
   },
   "outputs": [],
   "source": [
    "softmax_reg.predict([[5, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ... completed üòâüòòüòÆ‚Äçüí®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
