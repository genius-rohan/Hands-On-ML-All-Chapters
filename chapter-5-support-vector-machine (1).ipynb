{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-20T16:50:28.380630Z","iopub.execute_input":"2022-08-20T16:50:28.381033Z","iopub.status.idle":"2022-08-20T16:50:28.388834Z","shell.execute_reply.started":"2022-08-20T16:50:28.381001Z","shell.execute_reply":"2022-08-20T16:50:28.387434Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## CHAPTER 5\n# **SUPPORT VECTOR MACHINES**","metadata":{}},{"cell_type":"markdown","source":"A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning\nmodel, capable of performing linear or nonlinear classification, regression, and even\noutlier detection. It is one of the most popular models in Machine Learning, and anyone\ninterested in Machine Learning should have it in their toolbox.","metadata":{}},{"cell_type":"markdown","source":"# **Synopsys:**\n### 1. Linear SVM Classification\n### 2. Non Linear SVM Classification\n### 3. SVM Regression","metadata":{}},{"cell_type":"markdown","source":"# 1) üõ≥Ô∏è Linear SVM Classification","metadata":{}},{"cell_type":"markdown","source":"### Soft Margin Classification\nThe objective is to\nfind a good balance between keeping the street as large as possible and limiting the\nmargin violations (i.e., instances that end up in the middle of the street or even on the\nwrong side). This is called soft margin classification<br>\nIn Scikit-Learn‚Äôs SVM classes, you can control this balance using the C hyperparameter:\na smaller C value leads to a wider street but more margin violations. Figure 5-4\nshows the decision boundaries and margins of two soft margin SVM classifiers on a\nnonlinearly separable dataset. On the left, using a low C value the margin is quite\nlarge, but many instances end up on the street. On the right, using a high C value the\nclassifier makes fewer margin violations but ends up with a smaller margin. However,\nit seems likely that the first classifier will generalize better: in fact even on this training\nset it makes fewer prediction errors, since most of the margin violations are\nactually on the correct side of the decision boundary.\n","metadata":{}},{"cell_type":"markdown","source":"### Iris Data Model\nThe following Scikit-Learn code loads the iris dataset, scales the features, and then\ntrains a linear SVM model (using the LinearSVC class with C = 1 and the hinge loss\nfunction, described shortly) to detect Iris-Virginica flowers.","metadata":{}},{"cell_type":"code","source":"# Setting up Libraries and Data\n\nfrom sklearn import datasets\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\n\niris = datasets.load_iris()\nX = iris[\"data\"][:, (2, 3)] # loading petal length and width\ny = (iris[\"target\"]==2).astype(np.float64) # Iris_Vriginica\n","metadata":{"execution":{"iopub.status.busy":"2022-08-20T19:00:19.616957Z","iopub.execute_input":"2022-08-20T19:00:19.617749Z","iopub.status.idle":"2022-08-20T19:00:19.628038Z","shell.execute_reply.started":"2022-08-20T19:00:19.617700Z","shell.execute_reply":"2022-08-20T19:00:19.627097Z"},"trusted":true},"execution_count":325,"outputs":[]},{"cell_type":"code","source":"for i in range(len(y)):\n    if y[i]:\n        plt.scatter(X[i, 0], X[i, 1], c=\"r\")\n    else:\n        plt.scatter(X[i, 0], X[i, 1], c=\"b\")\nplt.title(\"Predicted\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-20T19:00:21.633605Z","iopub.execute_input":"2022-08-20T19:00:21.634934Z","iopub.status.idle":"2022-08-20T19:00:23.195744Z","shell.execute_reply.started":"2022-08-20T19:00:21.634885Z","shell.execute_reply":"2022-08-20T19:00:23.194452Z"},"trusted":true},"execution_count":326,"outputs":[]},{"cell_type":"code","source":"# USING linearSVC()\nsvm_clf = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\n])\nsvm_clf.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2022-08-20T19:00:25.111614Z","iopub.execute_input":"2022-08-20T19:00:25.112058Z","iopub.status.idle":"2022-08-20T19:00:25.124876Z","shell.execute_reply.started":"2022-08-20T19:00:25.112019Z","shell.execute_reply":"2022-08-20T19:00:25.123724Z"},"trusted":true},"execution_count":327,"outputs":[]},{"cell_type":"code","source":"pred_linear_svc = svm_clf.predict(X)\n\nfor i in range(len(pred_linear_svc)):\n    if pred_linear_svc[i]:\n        plt.scatter(X[i, 0], X[i, 1], c=\"r\", alpha=0.7)\n    else:\n        plt.scatter(X[i, 0], X[i, 1], c=\"b\", alpha=0.7)\nplt.title(\"Predicted\")\nplt.show()\n#Unlike Logistic Regression classifiers, SVM classifiers do not output probabilities for each class.","metadata":{"execution":{"iopub.status.busy":"2022-08-20T19:01:38.642681Z","iopub.execute_input":"2022-08-20T19:01:38.643299Z","iopub.status.idle":"2022-08-20T19:01:40.197419Z","shell.execute_reply.started":"2022-08-20T19:01:38.643262Z","shell.execute_reply":"2022-08-20T19:01:40.196385Z"},"trusted":true},"execution_count":330,"outputs":[]},{"cell_type":"markdown","source":"### Other Ways to implement SVC\nüëâ Alternatively, you could use the SVC class, using SVC(kernel=\"linear\", C=1), but it\nis much slower, especially with large training sets, so it is not recommended.<br><br>\nüëâ Another\noption is to use the SGDClassifier class, with SGDClassifier(loss=\"hinge\",\nalpha=1/(m*C)). This applies regular Stochastic Gradient Descent (see Chapter 4) to\ntrain a linear SVM classifier. It does not converge as fast as the LinearSVC class, but it\ncan be useful to handle huge datasets that do not fit in memory (out-of-core training),\nor to handle online classification tasks.","metadata":{}},{"cell_type":"markdown","source":"# 2) ü•¥ Non Linear SVM Classification","metadata":{}},{"cell_type":"markdown","source":"## a) Using Pipeline to add more features(polynomial)","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import make_moons\nfrom sklearn.preprocessing import PolynomialFeatures\n\npolynomial_svm_clf = Pipeline([\n    (\"poly_features\", PolynomialFeatures(degree=3)),\n    (\"scaler\", StandardScaler()),\n    (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\")),\n])\nmoons = make_moons()\nX_moons = np.array(moons[:][0])\ny_moons = np.array(moons[:][1])\nfor i in range(X_moons.shape[0]):\n    X_moons[i, 1] += (np.random.randn())/10\n    X_moons[i, 0] += (np.random.randn())/10\n\npolynomial_svm_clf.fit(X_moons, y_moons)\n\n# the model failed to converge as it fits the data in first step\n# the reason for this is the graph plotted below\n# but after adding noise, data is now slowly fitted","metadata":{"execution":{"iopub.status.busy":"2022-08-20T17:22:15.099045Z","iopub.execute_input":"2022-08-20T17:22:15.099493Z","iopub.status.idle":"2022-08-20T17:22:15.119229Z","shell.execute_reply.started":"2022-08-20T17:22:15.099455Z","shell.execute_reply":"2022-08-20T17:22:15.117963Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"predicted = polynomial_svm_clf.predict(X_moons)\nfor i in range(len(predicted)):\n    if predicted[i]:\n        plt.scatter(X_moons[i, 0], X_moons[i, 1], c=\"r\")\n    else:\n        plt.scatter(X_moons[i, 0], X_moons[i, 1], c=\"b\")\nplt.title(\"Predicted\")\nplt.show()\n#plt.scatter(X_moons[:, 0], X_moons[:, 1])\n","metadata":{"execution":{"iopub.status.busy":"2022-08-20T17:27:35.434134Z","iopub.execute_input":"2022-08-20T17:27:35.434612Z","iopub.status.idle":"2022-08-20T17:27:36.556862Z","shell.execute_reply.started":"2022-08-20T17:27:35.434572Z","shell.execute_reply":"2022-08-20T17:27:36.555492Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"markdown","source":"## b) Polynomial Kernel\nwhen using SVMs you can apply an almost miraculous mathematical\ntechnique called the kernel trick (it is explained in a moment). It makes it possible to\nget the same result as if you added many polynomial features, even with very highdegree\npolynomials, without actually having to add them","metadata":{}},{"cell_type":"code","source":"# applying polynomial kernel using sklearn\n\nfrom sklearn.svm import SVC\npoly_kernel_svm_clf = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n])\npoly_kernel_svm_clf.fit(X_moons, y_moons)\n\n# The hyperparameter coef0 controls how much the model is \n# influenced by highdegree\n# polynomials versus low-degree polynomials.","metadata":{"execution":{"iopub.status.busy":"2022-08-20T17:45:48.171807Z","iopub.execute_input":"2022-08-20T17:45:48.172243Z","iopub.status.idle":"2022-08-20T17:45:48.187470Z","shell.execute_reply.started":"2022-08-20T17:45:48.172210Z","shell.execute_reply":"2022-08-20T17:45:48.186440Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"# predicting the data \nall_data = np.random.randn(500,2)\npredicted = poly_kernel_svm_clf.predict(all_data)\nfor i in range(len(predicted)):\n    if predicted[i]:\n        plt.scatter(all_data[i, 0], all_data[i, 1], c=\"r\", s=10)\n    else:\n        plt.scatter(all_data[i, 0], all_data[i, 1], c=\"b\", s=10)\nplt.title(\"Predicted\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-20T17:45:48.683457Z","iopub.execute_input":"2022-08-20T17:45:48.684584Z","iopub.status.idle":"2022-08-20T17:45:53.541579Z","shell.execute_reply.started":"2022-08-20T17:45:48.684511Z","shell.execute_reply":"2022-08-20T17:45:53.540382Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"markdown","source":"## c) Adding Similarity Features\nAnother technique to tackle nonlinear problems is to add features computed using a\nsimilarity function that measures how much each instance resembles a particular\nlandmark. For example, let‚Äôs take the one-dimensional dataset discussed earlier and\nadd two landmarks to it at x1 = ‚Äì2 and x1 = 1 (see the left plot in Figure 5-8). Next,\nlet‚Äôs define the similarity function to be the Gaussian Radial Basis Function (RBF)\nwith Œ≥ = 0.3 .<br>\nIt is a bell-shaped function varying from 0 (very far away from the landmark) to 1 (at\nthe landmark). Now we are ready to compute the new features. For example, let‚Äôs look\nat the instance x1 = ‚Äì1: it is located at a distance of 1 from the first landmark, and 2\nfrom the second landmark. Therefore its new features are x2 = exp (‚Äì0.3 √ó 12) ‚âà 0.74\nand x3 = exp (‚Äì0.3 √ó 2<sup>2</sup>) ‚âà 0.30. The plot on the right of Figure 5-8 shows the transformed\ndataset (dropping the original features). As you can see, it is now linearly\nseparable.","metadata":{}},{"cell_type":"markdown","source":"#### Gaussian RBF Kernel\nJust like the polynomial features method, the similarity features method can be useful\nwith any Machine Learning algorithm, but it may be computationally expensive to\ncompute all the additional features, especially on large training sets. However, once\nagain the kernel trick does its SVM magic: it makes it possible to obtain a similar\nresult as if you had added many similarity features, without actually having to add\nthem. Let‚Äôs try the Gaussian RBF kernel using the SVC class:","metadata":{}},{"cell_type":"code","source":"# making pipelines\nrbf_kernel_svm_clf = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n])\nrbf_kernel_svm_clf.fit(X_moons, y_moons)","metadata":{"execution":{"iopub.status.busy":"2022-08-20T17:55:19.413638Z","iopub.execute_input":"2022-08-20T17:55:19.414027Z","iopub.status.idle":"2022-08-20T17:55:19.427821Z","shell.execute_reply.started":"2022-08-20T17:55:19.413996Z","shell.execute_reply":"2022-08-20T17:55:19.426378Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"# predicting and ploting \n\npredicted = rbf_kernel_svm_clf.predict(X_moons)\nfor i in range(len(predicted)):\n    if predicted[i]:\n        plt.scatter(X_moons[i, 0], X_moons[i, 1], c=\"r\")\n    else:\n        plt.scatter(X_moons[i, 0], X_moons[i, 1], c=\"b\")\nplt.title(\"Predicted\")\nplt.show()\n#plt.scatter(X_moons[:, 0], X_moons[:, 1])\n","metadata":{"execution":{"iopub.status.busy":"2022-08-20T17:55:19.813355Z","iopub.execute_input":"2022-08-20T17:55:19.814359Z","iopub.status.idle":"2022-08-20T17:55:20.892511Z","shell.execute_reply.started":"2022-08-20T17:55:19.814316Z","shell.execute_reply":"2022-08-20T17:55:20.891095Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"markdown","source":"### üìå TIP\n>use? As a rule of thumb, you should always try the linear\nkernel first (remember that LinearSVC is much faster than SVC(ker\nnel=\"linear\")), especially if the training set is very large or if it\nhas plenty of features. If the training set is not too large, you should\ntry the Gaussian RBF kernel as well; it works well in most cases.\nThen if you have spare time and computing power, you can also\nexperiment with a few other kernels using cross-validation and grid\nsearch, especially if there are kernels specialized for your training\nset‚Äôs data structure.","metadata":{}},{"cell_type":"markdown","source":"## Computational Complexity ‚åõ\n<table>\n    <tr><th>Class</th><th>Time complexity</th><th>Out-of-core support</th><th>Scaling required</th> <th>Kernel trick</th> </tr>    \n    <tr><td>LinearSVC</td><td> O(m √ó n)</td><td> No</td><td> Yes</td><td> No</td></tr>\n    <tr> <td> SGDClassifier</td><td> O(m √ó n)</td><td> Yes</td><td> Yes</td><td> No</td><td></tr>\n    <tr>   <td>SVC</td><td> O(m¬≤ √ó n) to O(m¬≥ √ó n)</td><td> No</td><td> Yes</td><td> Yes</td></tr>\n</table>","metadata":{}},{"cell_type":"markdown","source":"# 3) üíπ SVM Regression\nThe trick is to reverse the objective: instead of trying to fit the largest possible\nstreet between two classes while limiting margin violations, SVM Regression\ntries to fit as many instances as possible on the street while limiting margin violations\n(i.e., instances off the street). The width of the street is controlled by a hyperparameter\nœµ. Figure 5-10 shows two linear SVM Regression models trained on some random\nlinear data, one with a large margin (œµ = 1.5) and the other with a small margin (œµ =\n0.5).","metadata":{}},{"cell_type":"code","source":"# Creating the testing data\nm=200\nX_t =  6*np.random.randn(m, 1) \ny_t = 0.5 * X_t**2 + X_t + 2 + np.random.randn(m, 1)*10","metadata":{"execution":{"iopub.status.busy":"2022-08-20T18:46:32.283807Z","iopub.execute_input":"2022-08-20T18:46:32.284353Z","iopub.status.idle":"2022-08-20T18:46:32.291642Z","shell.execute_reply.started":"2022-08-20T18:46:32.284306Z","shell.execute_reply":"2022-08-20T18:46:32.290322Z"},"trusted":true},"execution_count":302,"outputs":[]},{"cell_type":"code","source":"plt.scatter(X_t, y_t, alpha=0.8)","metadata":{"execution":{"iopub.status.busy":"2022-08-20T18:47:38.313424Z","iopub.execute_input":"2022-08-20T18:47:38.314190Z","iopub.status.idle":"2022-08-20T18:47:38.534047Z","shell.execute_reply.started":"2022-08-20T18:47:38.314150Z","shell.execute_reply":"2022-08-20T18:47:38.532823Z"},"trusted":true},"execution_count":311,"outputs":[]},{"cell_type":"markdown","source":"### Using LinearSVR","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import LinearSVR\nsvm_reg = LinearSVR(epsilon=1.5)\nsvm_reg.fit(X_t, y_t.ravel())","metadata":{"execution":{"iopub.status.busy":"2022-08-20T18:46:37.863366Z","iopub.execute_input":"2022-08-20T18:46:37.863819Z","iopub.status.idle":"2022-08-20T18:46:37.874318Z","shell.execute_reply.started":"2022-08-20T18:46:37.863782Z","shell.execute_reply":"2022-08-20T18:46:37.873290Z"},"trusted":true},"execution_count":304,"outputs":[]},{"cell_type":"code","source":"prediction_svm = svm_reg.predict(X_t)\nplt.scatter(X_t, prediction_svm)\nplt.scatter(X_t, y_t, c=\"orange\", alpha=0.4)","metadata":{"execution":{"iopub.status.busy":"2022-08-20T18:47:10.333633Z","iopub.execute_input":"2022-08-20T18:47:10.334377Z","iopub.status.idle":"2022-08-20T18:47:10.560510Z","shell.execute_reply.started":"2022-08-20T18:47:10.334339Z","shell.execute_reply":"2022-08-20T18:47:10.559294Z"},"trusted":true},"execution_count":309,"outputs":[]},{"cell_type":"markdown","source":"### Using SVR","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVR\nsvm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=1)\nsvm_poly_reg.fit(X_t, y_t.ravel())\n","metadata":{"execution":{"iopub.status.busy":"2022-08-20T18:46:42.153566Z","iopub.execute_input":"2022-08-20T18:46:42.154006Z","iopub.status.idle":"2022-08-20T18:46:42.168811Z","shell.execute_reply.started":"2022-08-20T18:46:42.153963Z","shell.execute_reply":"2022-08-20T18:46:42.167571Z"},"trusted":true},"execution_count":306,"outputs":[]},{"cell_type":"code","source":"pred_svr = svm_poly_reg.predict(X_t)\nplt.scatter(X_t, y_t, c=\"orange\", alpha=0.7)\nplt.scatter(X_t, pred_svr)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-20T18:47:24.321506Z","iopub.execute_input":"2022-08-20T18:47:24.321936Z","iopub.status.idle":"2022-08-20T18:47:24.551409Z","shell.execute_reply.started":"2022-08-20T18:47:24.321901Z","shell.execute_reply":"2022-08-20T18:47:24.550628Z"},"trusted":true},"execution_count":310,"outputs":[]},{"cell_type":"markdown","source":"# üó∫Ô∏èCompleted !! üöµ‚Äç üé¢\n# ü§™ü•¥","metadata":{}}]}